{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "downtown-reservation",
   "metadata": {},
   "source": [
    "# 1. Loading partitioned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-25 19:31:41,092 - kedro.io.data_catalog - INFO - Loading data from `sgs_dataset` (PartitionedDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "sgs_dataset = catalog.load('sgs_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "underlying-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>xx</th>\n",
       "      <th>yy</th>\n",
       "      <th>zz</th>\n",
       "      <th>speed_x</th>\n",
       "      <th>speed_y</th>\n",
       "      <th>speed_z</th>\n",
       "      <th>speed_xx</th>\n",
       "      <th>...</th>\n",
       "      <th>rupture_time</th>\n",
       "      <th>session_id</th>\n",
       "      <th>swell_dir</th>\n",
       "      <th>swell_hs</th>\n",
       "      <th>swell_tp</th>\n",
       "      <th>wave_dir</th>\n",
       "      <th>wave_hs</th>\n",
       "      <th>wave_tp</th>\n",
       "      <th>win_dir</th>\n",
       "      <th>wind_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.300000</td>\n",
       "      <td>15.700000</td>\n",
       "      <td>-16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>244.50000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>201.2</td>\n",
       "      <td>2.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.316316</td>\n",
       "      <td>15.708154</td>\n",
       "      <td>-16.020592</td>\n",
       "      <td>-0.061946</td>\n",
       "      <td>0.011578</td>\n",
       "      <td>244.50244</td>\n",
       "      <td>0.005226</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>-0.039725</td>\n",
       "      <td>-0.120705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>201.2</td>\n",
       "      <td>2.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.359290</td>\n",
       "      <td>15.730342</td>\n",
       "      <td>-16.075703</td>\n",
       "      <td>-0.231323</td>\n",
       "      <td>0.035530</td>\n",
       "      <td>244.50719</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>-0.067912</td>\n",
       "      <td>-0.212014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>201.2</td>\n",
       "      <td>2.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416484</td>\n",
       "      <td>15.761158</td>\n",
       "      <td>-16.149070</td>\n",
       "      <td>-0.471763</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>244.50937</td>\n",
       "      <td>-0.000480</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>-0.075733</td>\n",
       "      <td>-0.261404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>201.2</td>\n",
       "      <td>2.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.478907</td>\n",
       "      <td>15.794098</td>\n",
       "      <td>-16.218199</td>\n",
       "      <td>-0.738104</td>\n",
       "      <td>0.057024</td>\n",
       "      <td>244.50912</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>-0.060947</td>\n",
       "      <td>-0.263505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.64</td>\n",
       "      <td>3.86</td>\n",
       "      <td>201.2</td>\n",
       "      <td>2.71</td>\n",
       "      <td>13.3</td>\n",
       "      <td>85.7</td>\n",
       "      <td>5.37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          x          y          z        xx        yy         zz   speed_x  \\\n",
       "0 -0.300000  15.700000 -16.000000  0.000000  0.000000  244.50000  0.000000   \n",
       "1 -0.316316  15.708154 -16.020592 -0.061946  0.011578  244.50244  0.005226   \n",
       "2 -0.359290  15.730342 -16.075703 -0.231323  0.035530  244.50719  0.004281   \n",
       "3 -0.416484  15.761158 -16.149070 -0.471763  0.053906  244.50937 -0.000480   \n",
       "4 -0.478907  15.794098 -16.218199 -0.738104  0.057024  244.50912 -0.002962   \n",
       "\n",
       "    speed_y   speed_z  speed_xx  ...  rupture_time  session_id  swell_dir  \\\n",
       "0  0.000000  0.000000  0.000000  ...           0.0          22       93.0   \n",
       "1 -0.001770 -0.039725 -0.120705  ...           0.0          22       93.0   \n",
       "2  0.000331 -0.067912 -0.212014  ...           0.0          22       93.0   \n",
       "3  0.003851 -0.075733 -0.261404  ...           0.0          22       93.0   \n",
       "4  0.003428 -0.060947 -0.263505  ...           0.0          22       93.0   \n",
       "\n",
       "   swell_hs  swell_tp  wave_dir  wave_hs  wave_tp  win_dir  wind_speed  \n",
       "0      0.64      3.86     201.2     2.71     13.3     85.7        5.37  \n",
       "1      0.64      3.86     201.2     2.71     13.3     85.7        5.37  \n",
       "2      0.64      3.86     201.2     2.71     13.3     85.7        5.37  \n",
       "3      0.64      3.86     201.2     2.71     13.3     85.7        5.37  \n",
       "4      0.64      3.86     201.2     2.71     13.3     85.7        5.37  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgs_dataset['22_0531_pos.csv']().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-patrick",
   "metadata": {},
   "source": [
    "# 2.Transforming Coordinate System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "permanent-bulgaria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import math as mt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple, Dict, Callable, Any, List, Union\n",
    "\n",
    "\n",
    "def transform_coordinates(\n",
    "        partitioned_input: Dict[str, Callable[[], Any]],\n",
    "    ) -> Dict[str, pd.DataFrame]:\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    for partition_key, partition_load_func in tqdm(\n",
    "            sorted(partitioned_input.items())\n",
    "        ):\n",
    "        partition_data = partition_load_func()  # load the actual partition data\n",
    "        result[partition_key] = apply_rotation_matrix(\n",
    "            partition_data['x'].values,\n",
    "            partition_data['y'].values,\n",
    "            partition_data['z'].values,\n",
    "            partition_data['xx'].values,\n",
    "            partition_data['yy'].values,\n",
    "            partition_data['zz'].values,\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def apply_rotation_matrix(\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        Z: np.ndarray,\n",
    "        XX: np.ndarray,\n",
    "        YY: np.ndarray,\n",
    "        ZZ: np.ndarray,\n",
    "        ignore_size: int = 500\n",
    "    ) -> np.ndarray:\n",
    "    '''\n",
    "    Applies the rotation matrix to transform absolute coordinates to\n",
    "    local coordinates.\n",
    "\n",
    "        Parameters:\n",
    "            X (np.ndarray): X in absolute coordinates\n",
    "            Y (np.ndarray): Y in absolute coordinates\n",
    "            Z (np.ndarray): Z in absolute coordinates\n",
    "            XX (np.ndarray): XX in absolute coordinates\n",
    "            YY (np.ndarray): YY in absolute coordinates\n",
    "            ZZ (np.ndarray): ZZ in absolute coordinates\n",
    "            ignore_size (:obj:`int`, optional): Steps to ignore in the\n",
    "                beggining of the series to remove transitive effects.\n",
    "                Default value is 500 points\n",
    "\n",
    "        Returns:\n",
    "            rotated (np.ndarray): local coordinates\n",
    "    '''\n",
    "    # Transform roll, pitch, and yaw to radians\n",
    "    roll = (XX*mt.pi)/180\n",
    "    pitch = (YY*mt.pi)/180\n",
    "    yaw = (ZZ*mt.pi)/180\n",
    "\n",
    "    # Apply rotation matrix to X and Y\n",
    "    x = X * np.cos(yaw) + Y * np.sin(yaw)\n",
    "    y = -X * np.sin(yaw) + Y * np.cos(yaw)\n",
    "\n",
    "    #Ignore the first points due to transitions effects\n",
    "    return pd.DataFrame({\n",
    "        'x': x[ignore_size:],\n",
    "        'y': y[ignore_size:],\n",
    "        'z': Z[ignore_size:],\n",
    "        'roll': roll[ignore_size:],\n",
    "        'pitch': pitch[ignore_size:],\n",
    "        'yaw': yaw[ignore_size:],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cubic-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:17<00:00,  5.83it/s]\n"
     ]
    }
   ],
   "source": [
    "transformed_sgs_dataset = transform_coordinates(\n",
    "    partitioned_input = sgs_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "interracial-asian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-13.766946</td>\n",
       "      <td>-8.285960</td>\n",
       "      <td>-16.280176</td>\n",
       "      <td>-0.013512</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>4.271540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-13.746646</td>\n",
       "      <td>-8.279071</td>\n",
       "      <td>-16.282722</td>\n",
       "      <td>-0.015320</td>\n",
       "      <td>0.000157</td>\n",
       "      <td>4.271894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-13.726923</td>\n",
       "      <td>-8.256329</td>\n",
       "      <td>-16.266628</td>\n",
       "      <td>-0.016726</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>4.272355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-13.706932</td>\n",
       "      <td>-8.220344</td>\n",
       "      <td>-16.240454</td>\n",
       "      <td>-0.017374</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>4.272714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-13.686217</td>\n",
       "      <td>-8.176853</td>\n",
       "      <td>-16.211172</td>\n",
       "      <td>-0.017050</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>4.272765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10296</th>\n",
       "      <td>-13.345962</td>\n",
       "      <td>-7.450245</td>\n",
       "      <td>-16.820381</td>\n",
       "      <td>-0.019447</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>4.272202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10297</th>\n",
       "      <td>-13.212179</td>\n",
       "      <td>-7.291303</td>\n",
       "      <td>-16.704054</td>\n",
       "      <td>-0.013375</td>\n",
       "      <td>-0.004215</td>\n",
       "      <td>4.270268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>-13.128090</td>\n",
       "      <td>-7.172471</td>\n",
       "      <td>-16.438196</td>\n",
       "      <td>-0.006742</td>\n",
       "      <td>-0.007402</td>\n",
       "      <td>4.267761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10299</th>\n",
       "      <td>-13.115679</td>\n",
       "      <td>-7.122371</td>\n",
       "      <td>-16.095242</td>\n",
       "      <td>-0.001158</td>\n",
       "      <td>-0.008409</td>\n",
       "      <td>4.265383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10300</th>\n",
       "      <td>-13.178809</td>\n",
       "      <td>-7.156863</td>\n",
       "      <td>-15.768992</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.007005</td>\n",
       "      <td>4.263744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10301 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               x         y          z      roll     pitch       yaw\n",
       "0     -13.766946 -8.285960 -16.280176 -0.013512  0.000118  4.271540\n",
       "1     -13.746646 -8.279071 -16.282722 -0.015320  0.000157  4.271894\n",
       "2     -13.726923 -8.256329 -16.266628 -0.016726  0.000487  4.272355\n",
       "3     -13.706932 -8.220344 -16.240454 -0.017374  0.000828  4.272714\n",
       "4     -13.686217 -8.176853 -16.211172 -0.017050  0.000886  4.272765\n",
       "...          ...       ...        ...       ...       ...       ...\n",
       "10296 -13.345962 -7.450245 -16.820381 -0.019447  0.000266  4.272202\n",
       "10297 -13.212179 -7.291303 -16.704054 -0.013375 -0.004215  4.270268\n",
       "10298 -13.128090 -7.172471 -16.438196 -0.006742 -0.007402  4.267761\n",
       "10299 -13.115679 -7.122371 -16.095242 -0.001158 -0.008409  4.265383\n",
       "10300 -13.178809 -7.156863 -15.768992  0.001991 -0.007005  4.263744\n",
       "\n",
       "[10301 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_sgs_dataset['22_0641_pos.csv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-manor",
   "metadata": {},
   "source": [
    "# 2. Creating Node to generate Feature Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaningful-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from kedro_mlflow_tutorial.utils.estimator import estimate_natural_period\n",
    "\n",
    "def generate_feature_data(\n",
    "        partitioned_input: Dict[str, Callable[[], Any]],\n",
    "        expected_tp: float,\n",
    "        target_column: 'str',\n",
    "        delta: float,\n",
    "        repetitions: int,\n",
    "        window_size: int,\n",
    "    ) -> pd.DataFrame:\n",
    "    '''\n",
    "    Generates the master table for training the regressor model, given\n",
    "    a partitioned dataset of time series. It estimates the natural period\n",
    "    of time series using the Welch's Method. It also calcuates statistics\n",
    "    of the time series.\n",
    "\n",
    "    Parameters:\n",
    "        partitioned_input (Dict[str, Callable[[], Any]]): kedro partitioned\n",
    "            dataset, which is dict of callables.\n",
    "        expected_tp (float): expected value for natural period\n",
    "        delta (float): the size of the segment used to filter\n",
    "            around the given center [center-delta,center+delta].\n",
    "        repetitions (int): number of repetitions to apply to\n",
    "            each window extracted from the time serie.\n",
    "        window_size (int): size of the window extracted from\n",
    "            the time serie.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (pd.DataFrame): generated data\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for partition_key, partition_load_func in tqdm(\n",
    "            sorted(partitioned_input.items())\n",
    "        ):\n",
    "        # Initializes regressor_data dict and sets partition key\n",
    "        feature_data = dict()\n",
    "        feature_data['partition_key'] = partition_key\n",
    "\n",
    "        # Loading data with the partition function\n",
    "        if isinstance(partition_load_func, pd.DataFrame):\n",
    "            partition_data = partition_load_func\n",
    "        else:\n",
    "            partition_data = partition_load_func()\n",
    "\n",
    "        # Calculating statistics\n",
    "        statistics_data = calculate_position_statistics(partition_data)\n",
    "        feature_data = {**feature_data, **statistics_data}\n",
    "\n",
    "        # Calculating natural period\n",
    "        feature_data[target_column], _, _, _, _ =  estimate_natural_period(\n",
    "            time_serie = partition_data[target_column.replace('tp_','')].values,\n",
    "            expected_tp = expected_tp,\n",
    "            delta = delta,\n",
    "            repetitions = repetitions,\n",
    "            window_size = window_size,\n",
    "        )\n",
    "\n",
    "        # Append generated data to final result\n",
    "        result.append(feature_data)\n",
    "\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "\n",
    "def calculate_position_statistics(\n",
    "        data: pd.DataFrame,\n",
    "    ):\n",
    "\n",
    "    return {\n",
    "        'off_x': np.mean(data['x'].values),\n",
    "        'off_y': np.mean(data['y'].values),\n",
    "        'off_z': np.mean(data['z'].values),\n",
    "        'off_roll': np.mean(data['roll'].values),\n",
    "        'off_pitch': np.mean(data['pitch'].values),\n",
    "        'off_yaw': np.mean(data['yaw'].values),\n",
    "        'std_x': np.std(data['x'].values),\n",
    "        'std_y': np.std(data['y'].values),\n",
    "        'std_z': np.std(data['z'].values),\n",
    "        'std_roll': np.std(data['roll'].values),\n",
    "        'std_pitch': np.std(data['pitch'].values),\n",
    "        'std_yaw': np.std(data['yaw'].values),\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "supposed-south",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-25 19:32:22,733 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.expected_tp` (MemoryDataSet)...\n",
      "2021-02-25 19:32:22,742 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.target_column` (MemoryDataSet)...\n",
      "2021-02-25 19:32:22,753 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.delta` (MemoryDataSet)...\n",
      "2021-02-25 19:32:22,757 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.repetitions` (MemoryDataSet)...\n",
      "2021-02-25 19:32:22,760 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.window_size` (MemoryDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:34<00:00,  2.88it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'master_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-b0047dd02e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmaster_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'master_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "feature_dataset = generate_feature_data(\n",
    "    partitioned_input = transformed_sgs_dataset,\n",
    "    expected_tp = catalog.load(\"params:estimator.expected_tp\"),\n",
    "    target_column = catalog.load(\"params:estimator.target_column\"),\n",
    "    delta = catalog.load(\"params:estimator.delta\"),\n",
    "    repetitions = catalog.load(\"params:estimator.repetitions\"),\n",
    "    window_size = catalog.load(\"params:estimator.window_size\"),\n",
    ")\n",
    "\n",
    "master_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-globe",
   "metadata": {},
   "source": [
    "# 3. Creating a Node to generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collectible-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def generate_training_data(\n",
    "        feature_dataset: pd.DataFrame,\n",
    "        target_column: str,\n",
    "        test_size: float,\n",
    "        valid_size: float,\n",
    "        shuffle: bool,\n",
    "    ):\n",
    "\n",
    "    # Generate Targets\n",
    "    X, Y = define_target_data(target_column, feature_dataset)\n",
    "\n",
    "    # Scaling the data\n",
    "    X_scaled, X_scaler, Y_scaled, Y_scaler = scale_regressor_data(X,Y)\n",
    "\n",
    "    # Spliting the data\n",
    "    X_train, X_valid, X_test,Y_train, Y_valid, Y_test = split_data(\n",
    "        X,\n",
    "        Y,\n",
    "        test_size,\n",
    "        valid_size,\n",
    "        shuffle,\n",
    "    )\n",
    "\n",
    "    return X_train, X_valid, X_test, X_scaler, Y_train, Y_valid, Y_test, Y_scaler\n",
    "\n",
    "\n",
    "def define_target_data(\n",
    "        target_column: str,\n",
    "        feature_dataset: pd.DataFrame\n",
    "    ) -> Tuple[pd.DataFrame]:\n",
    "\n",
    "    # Drop target column in X\n",
    "    X = feature_dataset.drop(target_column, axis=1)\n",
    "\n",
    "    # Get target data\n",
    "    Y = feature_dataset[target_column]\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def scale_regressor_data(\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "    ) -> Tuple[pd.DataFrame]:\n",
    "\n",
    "    # Set partition keys\n",
    "    partition_keys = X['partition_key']\n",
    "\n",
    "    # Scale X\n",
    "    X_scaled, X_scaler = scale_data(X.drop('partition_key', axis=1))\n",
    "\n",
    "    # Scale Y\n",
    "    Y_scaled, Y_scaler = scale_data(Y.values.reshape(-1,1))\n",
    "\n",
    "    X_scaled['partition_key'] = partition_keys\n",
    "\n",
    "    return X_scaled, X_scaler, Y_scaled, Y_scaler\n",
    "\n",
    "\n",
    "def scale_data(\n",
    "        data: Union[pd.DataFrame, np.ndarray],\n",
    "    ) -> Tuple:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        scaled_data = pd.DataFrame(\n",
    "            scaler.transform(data),\n",
    "            columns = data.columns,\n",
    "            index = data.index\n",
    "        )\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        scaled_data = pd.DataFrame(\n",
    "            scaler.transform(data),\n",
    "            columns=['y']\n",
    "        )\n",
    "\n",
    "    return scaled_data, scaler\n",
    "\n",
    "\n",
    "def split_data(\n",
    "        X: pd.DataFrame,\n",
    "        Y: pd.DataFrame,\n",
    "        test_size: float,\n",
    "        valid_size: float,\n",
    "        shuffle: bool,\n",
    "    ) -> Tuple[pd.DataFrame]:\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        Y,\n",
    "        test_size=test_size,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "\n",
    "    if valid_size:\n",
    "\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            test_size=valid_size/(1 - test_size),\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        return x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "    else:\n",
    "        return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "apparent-radar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-23 23:50:03,487 - kedro.io.data_catalog - INFO - Loading data from `params:estimator.target_column` (MemoryDataSet)...\n",
      "2021-02-23 23:50:03,488 - kedro.io.data_catalog - INFO - Loading data from `params:regressor.test_size` (MemoryDataSet)...\n",
      "2021-02-23 23:50:03,491 - kedro.io.data_catalog - INFO - Loading data from `params:regressor.valid_size` (MemoryDataSet)...\n",
      "2021-02-23 23:50:03,492 - kedro.io.data_catalog - INFO - Loading data from `params:regressor.shuffle` (MemoryDataSet)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user01/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, X_test, X_scaler, Y_train, Y_valid, Y_test, Y_scaler = generate_training_data(\n",
    "    feature_dataset = feature_dataset,\n",
    "    target_column = catalog.load(\"params:estimator.target_column\"),\n",
    "    test_size = catalog.load(\"params:regressor.test_size\"),\n",
    "    valid_size = catalog.load(\"params:regressor.valid_size\"),\n",
    "    shuffle = catalog.load(\"params:regressor.shuffle\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-royalty",
   "metadata": {},
   "source": [
    "# 5. Next steps\n",
    "\n",
    "- 1. Update **nodes.py** file for data integration pipeline\n",
    "- 2. Update **pipeline.py** file for data integration pipeline\n",
    "- 3. Update **hooks.py** file\n",
    "- 4. Update **conf/base/catalog.yml** file\n",
    "- 5. **Commit code to repo**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KedroMLFlowTutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
